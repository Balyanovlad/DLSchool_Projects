## Transformer(Bert) Classification

## Задача: 
- Решить задачу классификации математических текстов с помощью Transformer моделей. 
- Использовать Bert-подобные модели в качестве backbone, добавив голову классификации. 
- Проанализировать влияние заморозки весов backbone при дообучении. 
- Визуализировать карты внимания перед и до дообучения и проинтерпретировать их. 

## Ход работы: 
В данном исследовании были использованы следующие модели: cointegrated/rubert-tiny2, tbs17/MathBERT. Обе модели были дообучены на наших данных и показали сходимость. 
Идея визуализации голов attention интересная, но проинтерпретировать результаты сложно. Так как в берт подобных моделях 1 токен не равен одному слову. Из-за чего сложно улавливать связи, найденные моделью. 

## Результаты: 
Все модели были обучены и показали приемлемые результаты. Важно отметить, что для достижения лучшего качества модели можно было обучить до полной сходимости, но выбранного кол-ва эпох достаточно для сравнительного анализа. Лучшей моделью стала: cointegrated/rubert-tiny2 с замороженными весами backbone. 
Также, были визуализированы карты attention. На визуализациях можно видеть, какие отношения в предложениях наделяются большими весами.

## Стек технологий: 
datasets, evaluate, transformers, BertModel, torch, datasets, numpy, matplotlib
